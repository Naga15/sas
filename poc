from pydantic import BaseModel, ValidationError
import boto3
import yaml
import sys
import os
from typing import List
from datetime import datetime, timedelta


class Config(BaseModel):
    """
    Configs required for running this code.
    in this demo, it's provided via yaml file
    see.example.config.yaml for syntax
    And input/config will be validate via this class
    """

    """
    path in local file system that  FSX mounted

    Corret me if i was wrong about FSX.I think it's a virtual disk rather than a service
    service means  that can be changed via api call and remote host will do actions for you.
    virtual disk means should install drivers first and mount. then it becomes a local dir.
    """
    local_fsx_mount_path: str

    """
    bucket info of FSX backed up to s3, include
    aws_s3_region: aws region str,
    aws_s3_ak: aws access key, 
    aws_s3_sk: aws secret key
    s3_bucket_name: aws bucket name
    s3_backup_dir_prefix: 
        prefix in s3 bucket to search FSX backups, 
        if entire bucket, use empty ''
    """
    aws_s3_region: str
    s3_bucket_name: str
    s3_backup_dir_prefix: str = ''


class LocalFileMetaObject:
    """
    Every file found in local FSX path will be transformed to this object for later use.
    """

    def __init__(self, file_path: str, state: os.stat_result, fsx_root: str):
        self.index = file_path.replace(fsx_root, "")
        self.stat = state
        self.size = state.st_size
        self.mtime = datetime.fromtimestamp(state.st_mtime)

    @property
    def is_older_than_90_days(self) -> bool:
        timestamp_90_days_before = datetime.now() - timedelta(minutes=10)
        return self.mtime < timestamp_90_days_before

    def __repr__(self):
        return str(dict(
            index=self.index,
            mtime=self.mtime,
            size=self.size,
            is_older_than_90_days=self.is_older_than_90_days
        ))


class Worker:
    def __init__(self, config: Config):
        self.config = config

    @property
    def s3client(self):
        awsSession = boto3.session.Session(
            region_name=self.config.aws_s3_region,
            aws_session_token=None
        )
        return awsSession.client("s3")

    def get_file_info_in_fsx_dir(self):
        file_list: List[LocalFileMetaObject] = []
        for root, dirs, files in os.walk(self.config.local_fsx_mount_path, topdown=True, onerror=lambda x: print(x),
                                         followlinks=False):
            for file_name in files:
                file_path = os.path.join(root, file_name)
                file_info = LocalFileMetaObject(file_path=file_path, state=os.stat(file_path),
                                                fsx_root=self.config.local_fsx_mount_path)
                file_list.append(file_info)
        return file_list

    def list_file_in_s3_backups(self, ContinuationToken=None):
        """
        Return file list of  all backup files from s3
        """
        s3client = self.s3client
        s3_list_params = dict(
            Bucket=self.config.s3_bucket_name,
            Prefix=self.config.s3_backup_dir_prefix,
            MaxKeys=100
        )
        if ContinuationToken is not None:
            s3_list_params["ContinuationToken"] = ContinuationToken
        response = s3client.list_objects_v2(**s3_list_params)
        this_page_meta_info = [dict(key=x['Key'], size=x['Size']) for x in response['Contents']]
        if response['IsTruncated']:
            return this_page_meta_info + self.get_file_info_in_s3_back_ups(
                ContinuationToken=response['NextContinuationToken'])
        else:
            return this_page_meta_info

    def get_s3_file_info(self):
        """
        Get list of backup files from s3
        and then trassform to key:value map
        key: file_path_absolute_to_backup_root (s3_key - s3_backup_base_dir)
        value: file_size
        :return:
        """
        s3_file_lists = self.list_file_in_s3_backups()
        file_info = {}
        for file in s3_file_lists:
            file_path_abosolte_to_backup_root = file['key'].replace(self.config.s3_backup_dir_prefix, '')
            file_size = file['size']
            file_info[file_path_abosolte_to_backup_root] = file_size
        return file_info


if __name__ == "__main__":
    with open("config.yaml", "r") as f:
        config_data = yaml.safe_load(f.read())
    try:
        worker_config = Config(**config_data)
    except ValidationError as err:
        print(err)
        sys.exit(1)
    worker = Worker(config=worker_config)
    fsx_files = worker.get_file_info_in_fsx_dir()
    overdue_files = list(filter(lambda x: x.is_older_than_90_days, fsx_files))
    s3_backup_file_info = worker.get_s3_file_info()
    overdue_files_info = []
    for file in overdue_files:
        file_path = file.index
        file_info = {
            "path": file_path,
            "overdued": True
        }
        if file_path in s3_backup_file_info:
            file_info['backuped'] = True
            if file.size == s3_backup_file_info[file_path]:
                file_info["size_matched"] = True
            else:
                file_info["size_matched"] = False
        else:
            file_info['backuped'] = False
        overdue_files_info.append(file_info)
    # print(overdue_files)
    # print(s3_backup_file_info)
    print(overdue_files_info)
